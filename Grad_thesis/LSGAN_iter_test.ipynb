{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1410,
     "status": "ok",
     "timestamp": 1547759165030,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "76DG_1bf64GT",
    "outputId": "901d18fc-2089-4942-8535-d03b30a06363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1390,
     "status": "ok",
     "timestamp": 1547759165030,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "vkzAllIK66ms",
    "outputId": "95793c99-a6ac-40f6-c1fb-67f75a880de1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Colab Notebooks/Practice-ML/Grad_thesis\n"
     ]
    }
   ],
   "source": [
    "cd /content/gdrive/My Drive/Colab Notebooks/Practice-ML/Grad_thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46937,
     "status": "ok",
     "timestamp": 1547759210592,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "2L53fqQ67E7e",
    "outputId": "628e1b19-9b90-4407-f618-095ca2ce7572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.14.6)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.22.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.14.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas) (1.11.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2018.9)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.11.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.14.6)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (5.3.0)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (3.4.3.18)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.14.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install pillow\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fPE7EzB96zyT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "import argparse\n",
    "import time\n",
    "from statistics import mean, variance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models, transforms\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style\n",
    "import torchvision.utils as vutils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from seq_net_ver2 import weights_init, Generator, Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48895,
     "status": "ok",
     "timestamp": 1547759212587,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "zO4Zlp_46zyY",
    "outputId": "c23f7c24-a098-4efd-e492-f1feb39cbee9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26192f8a4f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定\n",
    "workers = 0\n",
    "batch_size=64\n",
    "nz = 100\n",
    "nch_g = 128\n",
    "nch_d = 128\n",
    "n_epoch = 50\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "outf = './Result/lsGAN'\n",
    "display_interval = 100\n",
    "\n",
    "# 保存先ディレクトリを作成\n",
    "try:\n",
    "    os.makedirs(outf, exist_ok=True)\n",
    "except OSError as error: \n",
    "    print(error)\n",
    "    pass\n",
    "\n",
    "# 乱数のシード（種）を固定\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pS2Hwb06zyd"
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 128\n",
    "\n",
    "def make_dataset(datadir):\n",
    "    '''\n",
    "    convert sequence to vector array\n",
    "    1.init array all 0 (4*SEQ_LENGTH)\n",
    "    2.convert sequences (all 0 array) to vector array.\n",
    "    ex. ACCGAT =\n",
    "    0 0 0 0 0 0    1 0 0 0 1 0\n",
    "    0 0 0 0 0 0  → 0 1 1 0 0 0\n",
    "    0 0 0 0 0 0    0 0 0 1 0 0\n",
    "    0 0 0 0 0 0    0 0 0 0 0 1\n",
    "    '''\n",
    "    pos_seq = \"SRX356455.05_peak_seq_128.txt\"\n",
    "    # id      chr     start   end     seq\n",
    "    data = pd.read_csv(os.path.join(datadir, \"sequences\", pos_seq), sep=\"\\t\")\n",
    "    sequences = [] \n",
    "    classes = [] #positive or negative\n",
    "    for index, row in data[[\"id\", \"seq\"]].iterrows():\n",
    "        y = 1 #positive\n",
    "        seq_vector = seq2vector(row[\"seq\"])\n",
    "        if len(seq_vector) == 0:\n",
    "            continue\n",
    "        sequences.append(seq2vector(row[\"seq\"]))\n",
    "        classes.append(np.array(y))\n",
    "    return sequences, classes\n",
    "\n",
    "def seq2vector(seq):\n",
    "    if type(seq) is not str: # Case on Null sequence\n",
    "        return np.zeros((0,0))\n",
    "    seq_array = np.zeros((4, SEQ_LENGTH)) #initiallize 4*SEQ_LENGTH array all 0\n",
    "    flag = 0\n",
    "    for i in range(SEQ_LENGTH):\n",
    "        s = seq[i]\n",
    "        if s == \"a\" or s == \"A\":\n",
    "            seq_array[0, i] = 1\n",
    "            seq_array[1, i] = 0\n",
    "            seq_array[2, i] = 0\n",
    "            seq_array[3, i] = 0\n",
    "        elif s == \"c\" or s == \"C\":\n",
    "            seq_array[0, i] = 0\n",
    "            seq_array[1, i] = 1\n",
    "            seq_array[2, i] = 0\n",
    "            seq_array[3, i] = 0\n",
    "        elif s == \"g\" or s == \"G\":\n",
    "            seq_array[0, i] = 0\n",
    "            seq_array[1, i] = 0\n",
    "            seq_array[2, i] = 1\n",
    "            seq_array[3, i] = 0\n",
    "        elif s == \"t\" or s == \"T\":\n",
    "            seq_array[0, i] = 0\n",
    "            seq_array[1, i] = 0\n",
    "            seq_array[2, i] = 0\n",
    "            seq_array[3, i] = 1\n",
    "        else:\n",
    "            flag += 1\n",
    "    if len(seq) == flag: # Case on N sequence\n",
    "        return np.zeros((0,0))\n",
    "    seq_array = seq_array.astype(np.float32)\n",
    "    return seq_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 80045,
     "status": "ok",
     "timestamp": 1547759243762,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "Q3F_XWIE6zyg",
    "outputId": "be6f1ed4-58d5-4382-bf74-5df47583877c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "datadir = \"data\"\n",
    "\n",
    "class DatasetFolder(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.samples = X\n",
    "        self.targets = y\n",
    "        self.transforms = transforms.Compose([\n",
    "            ToTensorOfTarget()\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        sample = self.transforms(sample)\n",
    "        target = self.targets[index]\n",
    "        target = self.transforms(target)\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "class ToTensorOfTarget(object):\n",
    "    def __call__(self, target):\n",
    "        return torch.from_numpy(target)\n",
    "\n",
    "# 全体を、training, valid, testに分ける。ここでは、3:1:1 に分割。\n",
    "# training + valid が、機械学習の training data 相当。\n",
    "X, y = make_dataset(datadir)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.01)\n",
    "\n",
    "\n",
    "sequence_datasets = {\n",
    "    'train':DatasetFolder(X_train, y_train),\n",
    "    'test': DatasetFolder(X_test, y_test)\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(sequence_datasets[x]) for x in ['train', 'test']}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LEJXjMwVG_pO"
   },
   "outputs": [],
   "source": [
    "# バッチサイズ分のデータを読み込む。\n",
    "# training はデータをシャッフルし、読み込み始める配列をランダムにする。\n",
    "# 他はシャッフルの必要なし。\n",
    "batch_size=64\n",
    "workers=0\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(\n",
    "        sequence_datasets['train'],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=workers),\n",
    "    'test': torch.utils.data.DataLoader(\n",
    "        sequence_datasets['test'],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=workers)\n",
    "}\n",
    "dataset_sizes = {x: len(sequence_datasets[x]) for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86262,
     "status": "ok",
     "timestamp": 1547759249997,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "THWJ0sN36zyk",
    "outputId": "d949d121-77fa-4106-c4a8-39a52414fa2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (layers): ModuleDict(\n",
      "    (layer0): Sequential(\n",
      "      (0): ConvTranspose1d(100, 2048, kernel_size=(4,), stride=(1,))\n",
      "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (0): ConvTranspose1d(2048, 1024, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): ConvTranspose1d(1024, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "      (3): Dropout(p=0.5)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): ConvTranspose1d(512, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): ConvTranspose1d(256, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "      (3): Dropout(p=0.5)\n",
      "    )\n",
      "    (layer5): Sequential(\n",
      "      (0): ConvTranspose1d(128, 4, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 生成器G。ランダムベクトルから贋作配列を生成する\n",
    "netG = Generator(nz=nz, nch_g=nch_g).to(device)\n",
    "netG.apply(weights_init)    # weights_init関数で初期化\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86685,
     "status": "ok",
     "timestamp": 1547759250430,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "zIsEJjBi6zyn",
    "outputId": "15be38a4-b909-46fa-e9f0-f43285d3979b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (layers): ModuleDict(\n",
      "    (layer0): Sequential(\n",
      "      (0): Conv1d(4, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (0): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Conv1d(512, 1024, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Conv1d(1024, 2048, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer5): Conv1d(2048, 1, kernel_size=(4,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 識別器D配列が、元配列か贋作配列かを識別する\n",
    "netD = Discriminator(nch_d=nch_d).to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abf1PyD96zyq"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()    # 損失関数は平均二乗誤差損失\n",
    "\n",
    "# オプティマイザ−のセットアップ\n",
    "optimizerD = optim.SGD(netD.parameters(), lr=lr, weight_decay=1e-5)  # 識別器D用\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, weight_decay=1e-5)  # 生成器G用\n",
    "\n",
    "fixed_noise = torch.randn(batch_size, nz, 1,  device=device)  # 確認用の固定したノイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1972
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1431,
     "status": "error",
     "timestamp": 1547757532865,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "dbEMPfif6zyt",
    "outputId": "86de345c-27ff-4713-bf12-91e13f2e0f1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50][1/843] Loss_D: 3.231 Loss_G: 16.984 D(x): 0.192 D(G(z)): 0.136/3.858\n",
      "D_mean is [4.283756438493729]\n",
      "G_mean is [7.943308498710394]\n",
      "[1/50][101/843] Loss_D: 1.288 Loss_G: 0.262 D(x): 0.808 D(G(z)): -0.267/0.487\n",
      "D_mean is [4.283756438493729, 2.8574264457821847]\n",
      "G_mean is [7.943308498710394, 4.767993453629315]\n",
      "[1/50][201/843] Loss_D: 1.083 Loss_G: 2.963 D(x): 1.144 D(G(z)): 0.707/-0.583\n",
      "D_mean is [4.283756438493729, 2.8574264457821847, 2.434421704610189]\n",
      "G_mean is [7.943308498710394, 4.767993453629315, 3.8662108276536067]\n",
      "[1/50][301/843] Loss_D: 1.091 Loss_G: 1.050 D(x): 0.858 D(G(z)): 0.729/-0.230\n",
      "D_mean is [4.283756438493729, 2.8574264457821847, 2.434421704610189, 2.085533424466848]\n",
      "G_mean is [7.943308498710394, 4.767993453629315, 3.8662108276536067, 3.2438281070254744]\n",
      "[1/50][401/843] Loss_D: 0.849 Loss_G: 2.183 D(x): 1.208 D(G(z)): 0.722/-0.489\n",
      "D_mean is [4.283756438493729, 2.8574264457821847, 2.434421704610189, 2.085533424466848, 1.853041258573532]\n",
      "G_mean is [7.943308498710394, 4.767993453629315, 3.8662108276536067, 3.2438281070254744, 2.8540852887257935]\n",
      "[1/50][501/843] Loss_D: 0.915 Loss_G: 0.867 D(x): 0.657 D(G(z)): 0.307/0.267\n",
      "D_mean is [4.283756438493729, 2.8574264457821847, 2.434421704610189, 2.085533424466848, 1.853041258573532, 1.6851914383471012]\n",
      "G_mean is [7.943308498710394, 4.767993453629315, 3.8662108276536067, 3.2438281070254744, 2.8540852887257935, 2.573940795256446]\n",
      "[1/50][601/843] Loss_D: 0.569 Loss_G: 0.118 D(x): 0.609 D(G(z)): -0.006/0.704\n",
      "D_mean is [4.283756438493729, 2.8574264457821847, 2.434421704610189, 2.085533424466848, 1.853041258573532, 1.6851914383471012, 1.5555891865491867]\n",
      "G_mean is [7.943308498710394, 4.767993453629315, 3.8662108276536067, 3.2438281070254744, 2.8540852887257935, 2.573940795256446, 2.373973142706922]\n",
      "[1/50][701/843] Loss_D: 0.669 Loss_G: 1.575 D(x): 1.166 D(G(z)): 0.568/-0.412\n",
      "D_mean is [4.283756438493729, 2.8574264457821847, 2.434421704610189, 2.085533424466848, 1.853041258573532, 1.6851914383471012, 1.5555891865491867, 1.444251210168004]\n",
      "G_mean is [7.943308498710394, 4.767993453629315, 3.8662108276536067, 3.2438281070254744, 2.8540852887257935, 2.573940795256446, 2.373973142706922, 2.205615585423075]\n",
      "[1/50][801/843] Loss_D: 0.363 Loss_G: 0.430 D(x): 0.589 D(G(z)): 0.071/0.060\n",
      "[2/50][1/843] Loss_D: 0.279 Loss_G: 0.448 D(x): 0.969 D(G(z)): 0.299/0.337\n",
      "D_mean is [4.283756438493729, 2.8574264457821847, 2.434421704610189, 2.085533424466848, 1.853041258573532, 1.6851914383471012, 1.5555891865491867, 1.444251210168004, 1.3395960849523545]\n",
      "G_mean is [7.943308498710394, 4.767993453629315, 3.8662108276536067, 3.2438281070254744, 2.8540852887257935, 2.573940795256446, 2.373973142706922, 2.205615585423075, 2.0573118790777194]\n",
      "[2/50][101/843] Loss_D: 0.689 Loss_G: 1.596 D(x): 1.170 D(G(z)): 0.529/-0.515\n",
      "D_mean is [4.283756438493729, 2.8574264457821847, 2.434421704610189, 2.085533424466848, 1.853041258573532, 1.6851914383471012, 1.5555891865491867, 1.444251210168004, 1.3395960849523545, 1.2629009070694446]\n",
      "G_mean is [7.943308498710394, 4.767993453629315, 3.8662108276536067, 3.2438281070254744, 2.8540852887257935, 2.573940795256446, 2.373973142706922, 2.205615585423075, 2.0573118790777194, 1.9499318293072283]\n",
      "[2/50][201/843] Loss_D: 0.307 Loss_G: 1.971 D(x): 1.093 D(G(z)): 0.335/-0.204\n",
      "D_mean is [4.283756438493729, 2.8574264457821847, 2.434421704610189, 2.085533424466848, 1.853041258573532, 1.6851914383471012, 1.5555891865491867, 1.444251210168004, 1.3395960849523545, 1.2629009070694446, 1.1870262612808835]\n",
      "G_mean is [7.943308498710394, 4.767993453629315, 3.8662108276536067, 3.2438281070254744, 2.8540852887257935, 2.573940795256446, 2.373973142706922, 2.205615585423075, 2.0573118790777194, 1.9499318293072283, 1.8606618264012715]\n",
      "[2/50][301/843] Loss_D: 0.289 Loss_G: 0.314 D(x): 0.862 D(G(z)): 0.105/0.672\n",
      "D_mean is [4.283756438493729, 2.8574264457821847, 2.434421704610189, 2.085533424466848, 1.853041258573532, 1.6851914383471012, 1.5555891865491867, 1.444251210168004, 1.3395960849523545, 1.2629009070694446, 1.1870262612808835, 1.1201933783168594]\n",
      "G_mean is [7.943308498710394, 4.767993453629315, 3.8662108276536067, 3.2438281070254744, 2.8540852887257935, 2.573940795256446, 2.373973142706922, 2.205615585423075, 2.0573118790777194, 1.9499318293072283, 1.8606618264012715, 1.7811588095904638]\n",
      "[2/50][401/843] Loss_D: 0.735 Loss_G: 0.451 D(x): 0.490 D(G(z)): -0.271/0.544\n",
      "D_mean is [4.283756438493729, 2.8574264457821847, 2.434421704610189, 2.085533424466848, 1.853041258573532, 1.6851914383471012, 1.5555891865491867, 1.444251210168004, 1.3395960849523545, 1.2629009070694446, 1.1870262612808835, 1.1201933783168594, 1.0624342752419986]\n",
      "G_mean is [7.943308498710394, 4.767993453629315, 3.8662108276536067, 3.2438281070254744, 2.8540852887257935, 2.573940795256446, 2.373973142706922, 2.205615585423075, 2.0573118790777194, 1.9499318293072283, 1.8606618264012715, 1.7811588095904638, 1.709811081304573]\n",
      "[2/50][501/843] Loss_D: 0.266 Loss_G: 0.728 D(x): 0.989 D(G(z)): 0.279/0.349\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-da57a0d4ab67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_image\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 更新した識別器Dで改めて贋作画像に対する識別信号を出力\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0merrG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_target\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# 生成器Gの損失値。Dに贋作画像を元画像と誤認させたいため目標値は「1」\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[1;31m#G_loss_sum += errG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mG_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#GeneratorのLossを格納\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\my-rdkit-env\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2154\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2155\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2156\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2157\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 学習のループ\n",
    "n_epoch = 50\n",
    "D_loss = []\n",
    "G_loss = []\n",
    "D_mean = []\n",
    "G_mean = []\n",
    "# D_loss_var = []\n",
    "for epoch in range(n_epoch):\n",
    "    #D_loss_sum = 0\n",
    "    #G_loss_sum = 0\n",
    "    for itr, data in enumerate(dataloaders['train']):\n",
    "        real_image = data[0].to(device)     # 元配列\n",
    "        #print(real_image)\n",
    "        sample_size = real_image.size(0)    # 配列数\n",
    "        noise = torch.randn(sample_size, nz, 1, device=device)   # 正規分布からノイズを生成\n",
    "        \n",
    "        real_target = torch.full((sample_size,), 1., device=device)     # 元配列に対する識別信号の目標値「1」\n",
    "        fake_target = torch.full((sample_size,), 0., device=device)     # 贋作配列に対する識別信号の目標値「0」\n",
    "        \n",
    "        ############################\n",
    "        # 識別器Dの更新\n",
    "        ###########################\n",
    "        netD.zero_grad()    # 勾配の初期化\n",
    "\n",
    "        output = netD(real_image)   # 識別器Dで元配列に対する識別信号を出力\n",
    "        errD_real = criterion(output, real_target)  # 元配列に対する識別信号の損失値\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        fake_image = netG(noise)    # 生成器Gでノイズから贋作配列を生成\n",
    "        #print(fake_image.detach())\n",
    "        output = netD(fake_image.detach())  # 識別器Dで元配列に対する識別信号を出力\n",
    "        errD_fake = criterion(output, fake_target)  # 贋作配列に対する識別信号の損失値\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        errD = errD_real + errD_fake    # 識別器Dの全体の損失\n",
    "        D_loss.append(errD.cpu().detach().numpy()) #DiscreminatorのLossを格納\n",
    "        errD.backward()    # 誤差逆伝播\n",
    "        optimizerD.step()   # Dのパラメーターを更新\n",
    "\n",
    "        ############################\n",
    "        # 生成器Gの更新\n",
    "        ###########################\n",
    "        netG.zero_grad()    # 勾配の初期化\n",
    "        \n",
    "        output = netD(fake_image)   # 更新した識別器Dで改めて贋作配列に対する識別信号を出力\n",
    "        errG = criterion(output, real_target)   # 生成器Gの損失値。Dに贋作配列を元画像と誤認させたいため目標値は「1」\n",
    "        #G_loss_sum += errG\n",
    "        G_loss.append(errG.cpu().detach().numpy()) #GeneratorのLossを格納\n",
    "        #print(errG.cpu().detach().numpy())\n",
    "        #print(\"G_sum is\",G_sum)\n",
    "        errG.backward()     # 誤差逆伝播\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()   # Gのパラメータを更新\n",
    "\n",
    "        if itr % display_interval == 0: \n",
    "            print('[{}/{}][{}/{}] Loss_D: {:.3f} Loss_G: {:.3f} D(x): {:.3f} D(G(z)): {:.3f}/{:.3f}'\n",
    "                  .format(epoch + 1, n_epoch,\n",
    "                          itr + 1, len(dataloaders['train']),\n",
    "                          errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            \n",
    "        if epoch == 0 and itr == 0:     # 初回に元配列を保存する\n",
    "            #sem.vec2seq_save(\"real_pos_seq\", real_image.cpu(), 1)\n",
    "            D_mean.append(mean(list(map(float, D_loss))))\n",
    "            G_mean.append(mean(list(map(float, G_loss))))\n",
    "            \n",
    "        if len(D_loss) % 1000 == 0:\n",
    "            D_mean.append(mean(list(map(float, D_loss))))\n",
    "            G_mean.append(mean(list(map(float, G_loss))))\n",
    "            print(\"D_mean is\",D_mean)\n",
    "            print(\"G_mean is\",G_mean)\n",
    "        #print(\"[\",itr,\"] SUM_lossD:\",D_loss_sum)\n",
    "        #print(\"[\",itr,\"] SUM_lossG:\",G_loss_sum)\n",
    "    ############################\n",
    "    # Lossの保存\n",
    "    ############################\n",
    "    #print(\"epoch\",epoch+1,\"D_loss is\",D_loss)\n",
    "    #print(\"epoch\",epoch+1,\"D_mean is\",mean(list(map(float, D_sum))))\n",
    "    #print(\"epoch\",epoch+1,\"G_loss is\",G_loss)\n",
    "    #print(\"epoch\",epoch+1,\"G_mean is\",mean(list(map(float, G_sum))))\n",
    "    #D_loss_var.append(variance(list(map(float, D_sum))))\n",
    "    #print(\"D_losses\",D_loss)\n",
    "    #print(\"G_losses\",G_loss)\n",
    "    #print(fake_image.detach())\n",
    "#     vutils.save_image(fake_image.detach(), '{}/fake_samples_epoch_{:03d}.png'.format(outf, epoch + 1),\n",
    "#                       normalize=True, nrow=10)\n",
    "\n",
    "    ############################\n",
    "    # 確認用の配列生成とその配列の保存\n",
    "    ############################\n",
    "        \n",
    "#     if (epoch + 1) % 5 == 0:   # 5エポックごとにfakeseqを保存する\n",
    "#         filename = 'fake_pos_seq_epoch' + str(epoch + 1)\n",
    "#         fake_image = netG(fixed_noise)  # 1エポック終了ごとに確認用の贋作配列を生成する\n",
    "#         sem.vec2seq_save(filename, fake_image.cpu().detach(), batch_size)\n",
    "#         torch.save(netG.state_dict(), '{}/netG_epoch_{}.pth'.format(outf, epoch + 1))\n",
    "#         torch.save(netD.state_dict(), '{}/netD_epoch_{}.pth'.format(outf, epoch + 1))\n",
    "\n",
    "############################\n",
    "# 結果の表示\n",
    "############################\n",
    "\n",
    "x_num = list(map(lambda x: x*1000, np.arange(0,len(D_mean))))\n",
    "#print(x_num)\n",
    "matplotlib.style.use('ggplot')\n",
    "fig1, Dx = plt.subplots()\n",
    "Dx.set_xlabel('iteration')\n",
    "Dx.set_ylabel('Dis/Loss')\n",
    "Dx.plot(x_num,D_mean,label='Discriminator Loss')\n",
    "Dx.legend(loc = 'best')\n",
    "Dx.set_title('')\n",
    "\n",
    "fig2, Gx = plt.subplots()\n",
    "Gx.set_xlabel('iteration')\n",
    "Gx.set_ylabel('Gen/Loss')\n",
    "Gx.plot(x_num,G_mean,label='Generater Loss')\n",
    "Gx.legend(loc = 'best')\n",
    "Gx.set_xlim(0, 54000)\n",
    "\n",
    "plt.xticks(np.arange(0, 54000 + 1, 10000))\n",
    "Gx.set_title('')\n",
    "plt.title('')\n",
    "\n",
    "plt.show\n",
    "\n",
    "fig1.savefig('iterDLosses.png')\n",
    "fig2.savefig('iterGLosses.png')\n",
    "\n",
    "print(\"Discriminator's variance is\",variance(list(map(float, D_loss))))\n",
    "print(\"Generater's variance is\",variance(list(map(float, G_loss))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jIOzZdu6zyw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSGAN_ver2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
