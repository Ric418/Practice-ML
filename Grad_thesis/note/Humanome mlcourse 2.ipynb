{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dir):\n",
    "    features = []\n",
    "    labels = []\n",
    "    # データの読み込みと特徴量の選択\n",
    "    dataset = pd.read_csv(os.path.join(dir, \"yeast_his3.csv\"))\n",
    "    columns = [\"C101\", \"C103\", \"C104\", \"C115\", \"A101\", \"A120\", \"A121\", \"A122\", \"A123\"]\n",
    "    cell_features_pre = dataset[[\"Cgroup\"] + columns]\n",
    "    cell_features = cell_features_pre[np.sum(cell_features_pre.isnull(), axis=1) == 0]\n",
    "    X = cell_features[columns]\n",
    "    groups = np.array(cell_features[\"Cgroup\"])\n",
    "    # 正規化\n",
    "    X_norm = preprocessing.StandardScaler().fit_transform(X)\n",
    "    # クラスの指定\n",
    "    for i in range(len(groups)):\n",
    "        group = groups[i]\n",
    "        feature = X_norm[i]\n",
    "        y = [0, 0, 0, 0]\n",
    "        if group == \"no\":\n",
    "            y = [1, 0, 0, 0]\n",
    "        elif group == \"small\":\n",
    "            y = [0, 1, 0, 0]\n",
    "        elif group == \"medium\":\n",
    "            y = [0, 0, 1, 0]\n",
    "        elif group == \"large\":\n",
    "            y = [0, 0, 0, 1]\n",
    "        features.append(np.array(feature.astype(np.float32)))\n",
    "        labels.append(np.array(y))\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# データの読み込み\n",
    "X, y = make_dataset(\"data\")\n",
    "# テストデータの分割\n",
    "X_tmp, X_test, y_tmp, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "# 訓練データとValidationデータの分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tmp, y_tmp, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetFolder(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.samples = X\n",
    "        self.targets = y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        target = self.targets[index]\n",
    "        sample = torch.from_numpy(sample)\n",
    "        target = torch.from_numpy(target)\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "feature_datasets = {\n",
    "    'train':DatasetFolder(X_train, y_train),\n",
    "    'val':DatasetFolder(X_val, y_val),\n",
    "    'test': DatasetFolder(X_test, y_test)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3911,  0.8372,  0.3250,  0.6466,  0.0129, -0.0699,  0.3997, -0.7443,\n",
      "        -1.0469])\n",
      "tensor([0, 1, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "sample, target = feature_datasets['train'][0]\n",
    "print(sample)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチサイズ分のデータを読み込む。\n",
    "# 訓練データ（train）はshuffle=Trueを指定することで、\n",
    "# データの順番をシャッフルし、読み込む順番をランダムにする。\n",
    "# 他はシャッフルの必要なし。\n",
    "batch_size=64\n",
    "workers=0\n",
    "dataloaders = {\n",
    "    # 訓練データ\n",
    "    'train': torch.utils.data.DataLoader(\n",
    "        feature_datasets['train'],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=workers),\n",
    "    # バリデーションデータ\n",
    "    'val': torch.utils.data.DataLoader(\n",
    "        feature_datasets['val'],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=workers),\n",
    "    # テストデータ\n",
    "    'test': torch.utils.data.DataLoader(\n",
    "        feature_datasets['test'],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=workers)\n",
    "}\n",
    "dataset_sizes = {x: len(feature_datasets[x]) for x in ['train', 'val', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 9)\n",
    "        self.fc2 = nn.Linear(9, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ここから先は、作成したネットワークを、指定のデバイスに送るための内容。\n",
    "# CPUではなく、GPUを利用したい場合は、\"cuda\" もしくは、\"cuda:0\" などと\n",
    "# 設定を記載。\n",
    "device_name = \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "model = Net()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    # 途中経過でモデル保存するための初期化\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    # 時間計測用\n",
    "    end = time.time()\n",
    "\n",
    "    print(model)\n",
    "    print()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch:{}/{}'.format(epoch, num_epochs - 1), end=\"\")\n",
    "\n",
    "        # 各エポックで訓練+バリデーションを実行\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # training mode\n",
    "            else:\n",
    "                model.train(False)  # evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                labels = labels.float()\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 訓練のときだけ履歴を保持する\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, classnums = torch.max(labels, 1)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, classnums)\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # 統計情報\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == classnums)\n",
    "\n",
    "            # サンプル数で割って平均を求める\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('\\t{} Loss: {:.4f} Acc: {:.4f} Time: {:.4f}'.format(phase, epoch_loss, epoch_acc, time.time()-end), end=\"\")\n",
    "\n",
    "            # 精度が改善したらモデルを保存する\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            end = time.time()\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print()\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val acc: {:.4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_accuracy(model, criterion, optimizer, phase):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train(False)\n",
    "\n",
    "    for inputs, labels in dataloaders[phase]:\n",
    "        labels = labels.float()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 訓練のときだけ履歴を保持する\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            outputs = model(inputs)\n",
    "            _, classnums = torch.max(labels, 1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, classnums)\n",
    "\n",
    "        # 統計情報\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == classnums)\n",
    "\n",
    "    # サンプル数で割って平均を求める\n",
    "    epoch_loss = running_loss / dataset_sizes[phase]\n",
    "    epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "    print('On Test:\\tLoss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=9, out_features=9, bias=True)\n",
      "  (fc2): Linear(in_features=9, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "Epoch:0/99\ttrain Loss: 0.4910 Acc: 0.8198 Time: 0.0050\tval Loss: 0.6161 Acc: 0.7067 Time: 0.0010\n",
      "Epoch:1/99\ttrain Loss: 0.4706 Acc: 0.8378 Time: 0.0050\tval Loss: 0.5978 Acc: 0.7333 Time: 0.0010\n",
      "Epoch:2/99\ttrain Loss: 0.4449 Acc: 0.8739 Time: 0.0040\tval Loss: 0.5845 Acc: 0.7333 Time: 0.0020\n",
      "Epoch:3/99\ttrain Loss: 0.4182 Acc: 0.8694 Time: 0.0040\tval Loss: 0.5646 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:4/99\ttrain Loss: 0.3860 Acc: 0.9009 Time: 0.0040\tval Loss: 0.5613 Acc: 0.7067 Time: 0.0010\n",
      "Epoch:5/99\ttrain Loss: 0.3600 Acc: 0.9144 Time: 0.0040\tval Loss: 0.5349 Acc: 0.7600 Time: 0.0020\n",
      "Epoch:6/99\ttrain Loss: 0.3336 Acc: 0.9189 Time: 0.0040\tval Loss: 0.5130 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:7/99\ttrain Loss: 0.3097 Acc: 0.9234 Time: 0.0040\tval Loss: 0.5012 Acc: 0.7600 Time: 0.0010\n",
      "Epoch:8/99\ttrain Loss: 0.2875 Acc: 0.9099 Time: 0.0040\tval Loss: 0.5061 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:9/99\ttrain Loss: 0.2688 Acc: 0.9234 Time: 0.0030\tval Loss: 0.5005 Acc: 0.7600 Time: 0.0010\n",
      "Epoch:10/99\ttrain Loss: 0.2545 Acc: 0.9414 Time: 0.0050\tval Loss: 0.5147 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:11/99\ttrain Loss: 0.2445 Acc: 0.9414 Time: 0.0040\tval Loss: 0.5171 Acc: 0.7333 Time: 0.0020\n",
      "Epoch:12/99\ttrain Loss: 0.2288 Acc: 0.9595 Time: 0.0040\tval Loss: 0.4861 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:13/99\ttrain Loss: 0.2254 Acc: 0.9505 Time: 0.0030\tval Loss: 0.4782 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:14/99\ttrain Loss: 0.2132 Acc: 0.9640 Time: 0.0040\tval Loss: 0.5194 Acc: 0.7333 Time: 0.0010\n",
      "Epoch:15/99\ttrain Loss: 0.2142 Acc: 0.9459 Time: 0.0030\tval Loss: 0.5027 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:16/99\ttrain Loss: 0.2001 Acc: 0.9685 Time: 0.0040\tval Loss: 0.4884 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:17/99\ttrain Loss: 0.1975 Acc: 0.9595 Time: 0.0030\tval Loss: 0.5173 Acc: 0.7200 Time: 0.0010\n",
      "Epoch:18/99\ttrain Loss: 0.1860 Acc: 0.9550 Time: 0.0040\tval Loss: 0.5145 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:19/99\ttrain Loss: 0.1869 Acc: 0.9595 Time: 0.0040\tval Loss: 0.4893 Acc: 0.7600 Time: 0.0010\n",
      "Epoch:20/99\ttrain Loss: 0.1770 Acc: 0.9730 Time: 0.0040\tval Loss: 0.4975 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:21/99\ttrain Loss: 0.1677 Acc: 0.9730 Time: 0.0040\tval Loss: 0.4965 Acc: 0.7333 Time: 0.0010\n",
      "Epoch:22/99\ttrain Loss: 0.1660 Acc: 0.9685 Time: 0.0040\tval Loss: 0.5041 Acc: 0.7333 Time: 0.0010\n",
      "Epoch:23/99\ttrain Loss: 0.1622 Acc: 0.9775 Time: 0.0040\tval Loss: 0.4906 Acc: 0.7600 Time: 0.0010\n",
      "Epoch:24/99\ttrain Loss: 0.1568 Acc: 0.9730 Time: 0.0040\tval Loss: 0.4991 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:25/99\ttrain Loss: 0.1590 Acc: 0.9685 Time: 0.0050\tval Loss: 0.5085 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:26/99\ttrain Loss: 0.1507 Acc: 0.9730 Time: 0.0040\tval Loss: 0.4944 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:27/99\ttrain Loss: 0.1507 Acc: 0.9730 Time: 0.0030\tval Loss: 0.5024 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:28/99\ttrain Loss: 0.1462 Acc: 0.9685 Time: 0.0040\tval Loss: 0.5306 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:29/99\ttrain Loss: 0.1444 Acc: 0.9775 Time: 0.0030\tval Loss: 0.5061 Acc: 0.7600 Time: 0.0010\n",
      "Epoch:30/99\ttrain Loss: 0.1412 Acc: 0.9775 Time: 0.0040\tval Loss: 0.4911 Acc: 0.7600 Time: 0.0010\n",
      "Epoch:31/99\ttrain Loss: 0.1444 Acc: 0.9775 Time: 0.0030\tval Loss: 0.4934 Acc: 0.7600 Time: 0.0010\n",
      "Epoch:32/99\ttrain Loss: 0.1372 Acc: 0.9775 Time: 0.0040\tval Loss: 0.4987 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:33/99\ttrain Loss: 0.1385 Acc: 0.9775 Time: 0.0030\tval Loss: 0.5125 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:34/99\ttrain Loss: 0.1335 Acc: 0.9775 Time: 0.0030\tval Loss: 0.4982 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:35/99\ttrain Loss: 0.1344 Acc: 0.9775 Time: 0.0040\tval Loss: 0.4973 Acc: 0.7600 Time: 0.0010\n",
      "Epoch:36/99\ttrain Loss: 0.1346 Acc: 0.9775 Time: 0.0030\tval Loss: 0.5134 Acc: 0.7733 Time: 0.0020\n",
      "Epoch:37/99\ttrain Loss: 0.1329 Acc: 0.9730 Time: 0.0050\tval Loss: 0.5382 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:38/99\ttrain Loss: 0.1278 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5230 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:39/99\ttrain Loss: 0.1274 Acc: 0.9775 Time: 0.0030\tval Loss: 0.5195 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:40/99\ttrain Loss: 0.1236 Acc: 0.9775 Time: 0.0040\tval Loss: 0.5374 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:41/99\ttrain Loss: 0.1245 Acc: 0.9730 Time: 0.0030\tval Loss: 0.5384 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:42/99\ttrain Loss: 0.1227 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5271 Acc: 0.7600 Time: 0.0011\n",
      "Epoch:43/99\ttrain Loss: 0.1200 Acc: 0.9820 Time: 0.0039\tval Loss: 0.5197 Acc: 0.7733 Time: 0.0020\n",
      "Epoch:44/99\ttrain Loss: 0.1212 Acc: 0.9775 Time: 0.0040\tval Loss: 0.5185 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:45/99\ttrain Loss: 0.1200 Acc: 0.9775 Time: 0.0039\tval Loss: 0.5205 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:46/99\ttrain Loss: 0.1186 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5258 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:47/99\ttrain Loss: 0.1180 Acc: 0.9775 Time: 0.0040\tval Loss: 0.5258 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:48/99\ttrain Loss: 0.1171 Acc: 0.9775 Time: 0.0030\tval Loss: 0.5235 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:49/99\ttrain Loss: 0.1154 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5267 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:50/99\ttrain Loss: 0.1152 Acc: 0.9820 Time: 0.0040\tval Loss: 0.5214 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:51/99\ttrain Loss: 0.1138 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5219 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:52/99\ttrain Loss: 0.1135 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5252 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:53/99\ttrain Loss: 0.1126 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5240 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:54/99\ttrain Loss: 0.1123 Acc: 0.9820 Time: 0.0040\tval Loss: 0.5304 Acc: 0.7600 Time: 0.0010\n",
      "Epoch:55/99\ttrain Loss: 0.1116 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5311 Acc: 0.7600 Time: 0.0010\n",
      "Epoch:56/99\ttrain Loss: 0.1115 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5278 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:57/99\ttrain Loss: 0.1108 Acc: 0.9820 Time: 0.0040\tval Loss: 0.5266 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:58/99\ttrain Loss: 0.1101 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5342 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:59/99\ttrain Loss: 0.1099 Acc: 0.9820 Time: 0.0040\tval Loss: 0.5333 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:60/99\ttrain Loss: 0.1088 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5295 Acc: 0.7733 Time: 0.0000\n",
      "Epoch:61/99\ttrain Loss: 0.1078 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5270 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:62/99\ttrain Loss: 0.1078 Acc: 0.9820 Time: 0.0040\tval Loss: 0.5249 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:63/99\ttrain Loss: 0.1079 Acc: 0.9820 Time: 0.0040\tval Loss: 0.5246 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:64/99\ttrain Loss: 0.1071 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5272 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:65/99\ttrain Loss: 0.1066 Acc: 0.9820 Time: 0.0040\tval Loss: 0.5316 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:66/99\ttrain Loss: 0.1065 Acc: 0.9820 Time: 0.0040\tval Loss: 0.5330 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:67/99\ttrain Loss: 0.1060 Acc: 0.9820 Time: 0.0040\tval Loss: 0.5341 Acc: 0.7733 Time: 0.0000\n",
      "Epoch:68/99\ttrain Loss: 0.1058 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5344 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:69/99\ttrain Loss: 0.1049 Acc: 0.9820 Time: 0.0030\tval Loss: 0.5373 Acc: 0.7600 Time: 0.0010\n",
      "Epoch:70/99\ttrain Loss: 0.1050 Acc: 0.9865 Time: 0.0030\tval Loss: 0.5385 Acc: 0.7467 Time: 0.0010\n",
      "Epoch:71/99\ttrain Loss: 0.1053 Acc: 0.9865 Time: 0.0040\tval Loss: 0.5370 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:72/99\ttrain Loss: 0.1044 Acc: 0.9865 Time: 0.0040\tval Loss: 0.5352 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:73/99\ttrain Loss: 0.1040 Acc: 0.9865 Time: 0.0030\tval Loss: 0.5355 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:74/99\ttrain Loss: 0.1038 Acc: 0.9865 Time: 0.0040\tval Loss: 0.5360 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:75/99\ttrain Loss: 0.1035 Acc: 0.9865 Time: 0.0040\tval Loss: 0.5359 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:76/99\ttrain Loss: 0.1037 Acc: 0.9865 Time: 0.0040\tval Loss: 0.5348 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:77/99\ttrain Loss: 0.1031 Acc: 0.9865 Time: 0.0030\tval Loss: 0.5347 Acc: 0.7733 Time: 0.0020\n",
      "Epoch:78/99\ttrain Loss: 0.1026 Acc: 0.9865 Time: 0.0040\tval Loss: 0.5356 Acc: 0.7733 Time: 0.0020\n",
      "Epoch:79/99\ttrain Loss: 0.1027 Acc: 0.9820 Time: 0.0040\tval Loss: 0.5375 Acc: 0.7600 Time: 0.0020\n",
      "Epoch:80/99\ttrain Loss: 0.1025 Acc: 0.9865 Time: 0.0050\tval Loss: 0.5356 Acc: 0.7600 Time: 0.0010\n",
      "Epoch:81/99\ttrain Loss: 0.1022 Acc: 0.9820 Time: 0.0050\tval Loss: 0.5337 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:82/99\ttrain Loss: 0.1019 Acc: 0.9865 Time: 0.0040\tval Loss: 0.5322 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:83/99\ttrain Loss: 0.1017 Acc: 0.9865 Time: 0.0030\tval Loss: 0.5311 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:84/99\ttrain Loss: 0.1015 Acc: 0.9865 Time: 0.0030\tval Loss: 0.5315 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:85/99\ttrain Loss: 0.1014 Acc: 0.9865 Time: 0.0040\tval Loss: 0.5312 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:86/99\ttrain Loss: 0.1011 Acc: 0.9865 Time: 0.0030\tval Loss: 0.5322 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:87/99\ttrain Loss: 0.1010 Acc: 0.9865 Time: 0.0040\tval Loss: 0.5328 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:88/99\ttrain Loss: 0.1009 Acc: 0.9865 Time: 0.0040\tval Loss: 0.5348 Acc: 0.7733 Time: 0.0020\n",
      "Epoch:89/99\ttrain Loss: 0.1008 Acc: 0.9865 Time: 0.0050\tval Loss: 0.5380 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:90/99\ttrain Loss: 0.1005 Acc: 0.9865 Time: 0.0030\tval Loss: 0.5394 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:91/99\ttrain Loss: 0.1004 Acc: 0.9865 Time: 0.0040\tval Loss: 0.5407 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:92/99\ttrain Loss: 0.1003 Acc: 0.9865 Time: 0.0030\tval Loss: 0.5406 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:93/99\ttrain Loss: 0.1002 Acc: 0.9865 Time: 0.0030\tval Loss: 0.5394 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:94/99\ttrain Loss: 0.1001 Acc: 0.9865 Time: 0.0040\tval Loss: 0.5389 Acc: 0.7733 Time: 0.0000\n",
      "Epoch:95/99\ttrain Loss: 0.1000 Acc: 0.9865 Time: 0.0030\tval Loss: 0.5386 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:96/99\ttrain Loss: 0.0999 Acc: 0.9865 Time: 0.0030\tval Loss: 0.5397 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:97/99\ttrain Loss: 0.0998 Acc: 0.9865 Time: 0.0030\tval Loss: 0.5401 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:98/99\ttrain Loss: 0.0997 Acc: 0.9865 Time: 0.0040\tval Loss: 0.5409 Acc: 0.7733 Time: 0.0010\n",
      "Epoch:99/99\ttrain Loss: 0.0995 Acc: 0.9865 Time: 0.0030\tval Loss: 0.5405 Acc: 0.7733 Time: 0.0010\n",
      "\n",
      "Training complete in 0m 0s\n",
      "Best val acc: 0.7733\n",
      "On Test:\tLoss: 0.3044 Acc: 0.8800\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "outdir = \".\"\n",
    "\n",
    "# 損失関数（クロスエントロピー）、\n",
    "# パラメータの最適化方法、学習率の更新方法を定義。\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)\n",
    "\n",
    "# 実際の学習を実施\n",
    "model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=epochs)\n",
    "# テストデータでの精度を求める\n",
    "print_test_accuracy(model, criterion, optimizer, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
