{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1410,
     "status": "ok",
     "timestamp": 1547759165030,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "76DG_1bf64GT",
    "outputId": "901d18fc-2089-4942-8535-d03b30a06363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1390,
     "status": "ok",
     "timestamp": 1547759165030,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "vkzAllIK66ms",
    "outputId": "95793c99-a6ac-40f6-c1fb-67f75a880de1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Colab Notebooks/Practice-ML/Grad_thesis\n"
     ]
    }
   ],
   "source": [
    "cd /content/gdrive/My Drive/Colab Notebooks/Practice-ML/Grad_thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46937,
     "status": "ok",
     "timestamp": 1547759210592,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "2L53fqQ67E7e",
    "outputId": "628e1b19-9b90-4407-f618-095ca2ce7572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.14.6)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.22.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.14.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas) (1.11.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2018.9)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.11.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.14.6)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (5.3.0)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (3.4.3.18)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.14.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install pillow\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fPE7EzB96zyT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import copy\n",
    "import argparse\n",
    "import time\n",
    "from statistics import mean, variance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import models, transforms\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style\n",
    "import torchvision.utils as vutils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from seq_net_ver2 import weights_init, Generator, Discriminator\n",
    "import seq_modules as sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48895,
     "status": "ok",
     "timestamp": 1547759212587,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "zO4Zlp_46zyY",
    "outputId": "c23f7c24-a098-4efd-e492-f1feb39cbee9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21f4c5ab630>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定\n",
    "workers = 0\n",
    "batch_size=64\n",
    "nz = 100\n",
    "nch_g = 128\n",
    "nch_d = 128\n",
    "n_epoch = 50\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "outf = './Result/lsGAN'\n",
    "display_interval = 100\n",
    "\n",
    "# 保存先ディレクトリを作成\n",
    "try:\n",
    "    os.makedirs(outf, exist_ok=True)\n",
    "except OSError as error: \n",
    "    print(error)\n",
    "    pass\n",
    "\n",
    "# 乱数のシード（種）を固定\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pS2Hwb06zyd"
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 128\n",
    "\n",
    "def make_dataset(datadir):\n",
    "    '''\n",
    "    convert sequence to vector array\n",
    "    1.init array all 0 (4*SEQ_LENGTH)\n",
    "    2.convert sequences (all 0 array) to vector array.\n",
    "    ex. ACCGAT =\n",
    "    0 0 0 0 0 0    1 0 0 0 1 0\n",
    "    0 0 0 0 0 0  → 0 1 1 0 0 0\n",
    "    0 0 0 0 0 0    0 0 0 1 0 0\n",
    "    0 0 0 0 0 0    0 0 0 0 0 1\n",
    "    '''\n",
    "    pos_seq = \"SRX356455.05_peak_seq_128.txt\"\n",
    "    # id      chr     start   end     seq\n",
    "    data = pd.read_csv(os.path.join(datadir, \"sequences\", pos_seq), sep=\"\\t\")\n",
    "    sequences = [] \n",
    "    classes = [] #positive or negative\n",
    "    for index, row in data[[\"id\", \"seq\"]].iterrows():\n",
    "        y = 1 #positive\n",
    "        seq_vector = seq2vector(row[\"seq\"])\n",
    "        if len(seq_vector) == 0:\n",
    "            continue\n",
    "        sequences.append(seq2vector(row[\"seq\"]))\n",
    "        classes.append(np.array(y))\n",
    "    return sequences, classes\n",
    "\n",
    "def seq2vector(seq):\n",
    "    if type(seq) is not str: # Case on Null sequence\n",
    "        return np.zeros((0,0))\n",
    "    seq_array = np.zeros((4, SEQ_LENGTH)) #initiallize 4*SEQ_LENGTH array all 0\n",
    "    flag = 0\n",
    "    for i in range(SEQ_LENGTH):\n",
    "        s = seq[i]\n",
    "        if s == \"a\" or s == \"A\":\n",
    "            seq_array[0, i] = 1\n",
    "            seq_array[1, i] = -1\n",
    "            seq_array[2, i] = -1\n",
    "            seq_array[3, i] = -1\n",
    "        elif s == \"c\" or s == \"C\":\n",
    "            seq_array[0, i] = -1\n",
    "            seq_array[1, i] = 1\n",
    "            seq_array[2, i] = -1\n",
    "            seq_array[3, i] = -1\n",
    "        elif s == \"g\" or s == \"G\":\n",
    "            seq_array[0, i] = -1\n",
    "            seq_array[1, i] = -1\n",
    "            seq_array[2, i] = 1\n",
    "            seq_array[3, i] = -1\n",
    "        elif s == \"t\" or s == \"T\":\n",
    "            seq_array[0, i] = -1\n",
    "            seq_array[1, i] = -1\n",
    "            seq_array[2, i] = -1\n",
    "            seq_array[3, i] = 1\n",
    "        else:\n",
    "            flag += 1\n",
    "    if len(seq) == flag: # Case on N sequence\n",
    "        return np.zeros((0,0))\n",
    "    seq_array = seq_array.astype(np.float32)\n",
    "    return seq_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 80045,
     "status": "ok",
     "timestamp": 1547759243762,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "Q3F_XWIE6zyg",
    "outputId": "be6f1ed4-58d5-4382-bf74-5df47583877c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "datadir = \"data\"\n",
    "\n",
    "class DatasetFolder(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.samples = X\n",
    "        self.targets = y\n",
    "        self.transforms = transforms.Compose([\n",
    "            ToTensorOfTarget()\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        sample = self.transforms(sample)\n",
    "        target = self.targets[index]\n",
    "        target = self.transforms(target)\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "\n",
    "class ToTensorOfTarget(object):\n",
    "    def __call__(self, target):\n",
    "        return torch.from_numpy(target)\n",
    "\n",
    "# 全体を、training, valid, testに分ける。ここでは、3:1:1 に分割。\n",
    "# training + valid が、機械学習の training data 相当。\n",
    "X, y = make_dataset(datadir)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.01)\n",
    "\n",
    "\n",
    "sequence_datasets = {\n",
    "    'train':DatasetFolder(X_train, y_train),\n",
    "    'test': DatasetFolder(X_test, y_test)\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(sequence_datasets[x]) for x in ['train', 'test']}\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LEJXjMwVG_pO"
   },
   "outputs": [],
   "source": [
    "# バッチサイズ分のデータを読み込む。\n",
    "# training はデータをシャッフルし、読み込み始める配列をランダムにする。\n",
    "# 他はシャッフルの必要なし。\n",
    "batch_size=64\n",
    "workers=0\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(\n",
    "        sequence_datasets['train'],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=workers),\n",
    "    'test': torch.utils.data.DataLoader(\n",
    "        sequence_datasets['test'],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=workers)\n",
    "}\n",
    "dataset_sizes = {x: len(sequence_datasets[x]) for x in ['train', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86262,
     "status": "ok",
     "timestamp": 1547759249997,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "THWJ0sN36zyk",
    "outputId": "d949d121-77fa-4106-c4a8-39a52414fa2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (layers): ModuleDict(\n",
      "    (layer0): Sequential(\n",
      "      (0): ConvTranspose1d(100, 2048, kernel_size=(4,), stride=(1,))\n",
      "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (0): ConvTranspose1d(2048, 1024, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): ConvTranspose1d(1024, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "      (3): Dropout(p=0.5)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): ConvTranspose1d(512, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): ConvTranspose1d(256, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "      (3): Dropout(p=0.5)\n",
      "    )\n",
      "    (layer5): Sequential(\n",
      "      (0): ConvTranspose1d(128, 4, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 生成器G。ランダムベクトルから贋作画像を生成する\n",
    "netG = Generator(nz=nz, nch_g=nch_g).to(device)\n",
    "netG.apply(weights_init)    # weights_init関数で初期化\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 86685,
     "status": "ok",
     "timestamp": 1547759250430,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "zIsEJjBi6zyn",
    "outputId": "15be38a4-b909-46fa-e9f0-f43285d3979b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (layers): ModuleDict(\n",
      "    (layer0): Sequential(\n",
      "      (0): Conv1d(4, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer1): Sequential(\n",
      "      (0): Conv1d(128, 256, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Conv1d(256, 512, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Conv1d(512, 1024, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Conv1d(1024, 2048, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "      (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): LeakyReLU(negative_slope=0.2)\n",
      "    )\n",
      "    (layer5): Conv1d(2048, 1, kernel_size=(4,), stride=(1,))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 識別器D。画像が、元画像か贋作画像かを識別する\n",
    "netD = Discriminator(nch_d=nch_d).to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "abf1PyD96zyq"
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()    # 損失関数は平均二乗誤差損失\n",
    "\n",
    "# オプティマイザ−のセットアップ\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)  # 識別器D用\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999), weight_decay=1e-5)  # 生成器G用\n",
    "\n",
    "fixed_noise = torch.randn(batch_size, nz, 1,  device=device)  # 確認用の固定したノイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1972
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1431,
     "status": "error",
     "timestamp": 1547757532865,
     "user": {
      "displayName": "Takuma Kobayashi",
      "photoUrl": "https://lh6.googleusercontent.com/-p-aKe8JYv6k/AAAAAAAAAAI/AAAAAAAAAB4/GxwcHgOFD6c/s64/photo.jpg",
      "userId": "06358397516930507128"
     },
     "user_tz": -540
    },
    "id": "dbEMPfif6zyt",
    "outputId": "86de345c-27ff-4713-bf12-91e13f2e0f1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/50][1/843] Loss_D: 3.231 Loss_G: 16.984 D(x): 0.192 D(G(z)): 0.136/3.858\n",
      "[1/50][101/843] Loss_D: 1.288 Loss_G: 0.262 D(x): 0.808 D(G(z)): -0.267/0.487\n",
      "[1/50][201/843] Loss_D: 1.083 Loss_G: 2.963 D(x): 1.144 D(G(z)): 0.707/-0.583\n",
      "[1/50][301/843] Loss_D: 1.091 Loss_G: 1.050 D(x): 0.858 D(G(z)): 0.729/-0.230\n",
      "[1/50][401/843] Loss_D: 0.849 Loss_G: 2.183 D(x): 1.208 D(G(z)): 0.722/-0.489\n",
      "[1/50][501/843] Loss_D: 0.915 Loss_G: 0.867 D(x): 0.657 D(G(z)): 0.307/0.267\n",
      "[1/50][601/843] Loss_D: 0.569 Loss_G: 0.118 D(x): 0.609 D(G(z)): -0.006/0.704\n",
      "[1/50][701/843] Loss_D: 0.669 Loss_G: 1.575 D(x): 1.166 D(G(z)): 0.568/-0.412\n",
      "[1/50][801/843] Loss_D: 0.363 Loss_G: 0.430 D(x): 0.589 D(G(z)): 0.071/0.060\n",
      "[2/50][1/843] Loss_D: 0.279 Loss_G: 0.448 D(x): 0.969 D(G(z)): 0.299/0.337\n",
      "[2/50][101/843] Loss_D: 0.689 Loss_G: 1.596 D(x): 1.170 D(G(z)): 0.529/-0.515\n",
      "D_mean is [3.2306833267211914, 1.2629009070694446]\n",
      "G_mean is [16.984115600585938, 1.9499318293072283]\n",
      "[2/50][201/843] Loss_D: 0.307 Loss_G: 1.971 D(x): 1.093 D(G(z)): 0.335/-0.204\n",
      "[2/50][301/843] Loss_D: 0.289 Loss_G: 0.314 D(x): 0.862 D(G(z)): 0.105/0.672\n",
      "[2/50][401/843] Loss_D: 0.735 Loss_G: 0.451 D(x): 0.490 D(G(z)): -0.271/0.544\n",
      "[2/50][501/843] Loss_D: 0.266 Loss_G: 0.728 D(x): 0.989 D(G(z)): 0.279/0.349\n",
      "[2/50][601/843] Loss_D: 0.210 Loss_G: 1.409 D(x): 1.375 D(G(z)): 0.480/0.028\n",
      "[2/50][701/843] Loss_D: 0.177 Loss_G: 0.849 D(x): 0.834 D(G(z)): 0.182/0.217\n",
      "[2/50][801/843] Loss_D: 0.162 Loss_G: 1.177 D(x): 1.102 D(G(z)): 0.409/0.085\n",
      "[3/50][1/843] Loss_D: 0.098 Loss_G: 0.873 D(x): 1.021 D(G(z)): 0.230/0.227\n",
      "[3/50][101/843] Loss_D: 0.400 Loss_G: 0.486 D(x): 0.744 D(G(z)): -0.245/0.496\n",
      "[3/50][201/843] Loss_D: 0.125 Loss_G: 0.517 D(x): 1.148 D(G(z)): 0.496/0.348\n",
      "[3/50][301/843] Loss_D: 0.313 Loss_G: 1.141 D(x): 1.429 D(G(z)): 0.711/0.140\n",
      "D_mean is [3.2306833267211914, 1.2629009070694446, 0.7730329616516829]\n",
      "G_mean is [16.984115600585938, 1.9499318293072283, 1.3754797047954053]\n",
      "[3/50][401/843] Loss_D: 0.147 Loss_G: 0.747 D(x): 0.811 D(G(z)): -0.133/0.155\n",
      "[3/50][501/843] Loss_D: 0.050 Loss_G: 0.599 D(x): 0.930 D(G(z)): 0.191/0.073\n",
      "[3/50][601/843] Loss_D: 0.249 Loss_G: 1.014 D(x): 1.196 D(G(z)): 0.479/0.028\n",
      "[3/50][701/843] Loss_D: 0.122 Loss_G: 0.281 D(x): 0.672 D(G(z)): -0.034/0.321\n",
      "[3/50][801/843] Loss_D: 0.137 Loss_G: 0.673 D(x): 0.823 D(G(z)): 0.066/0.310\n",
      "[4/50][1/843] Loss_D: 0.071 Loss_G: 0.845 D(x): 0.985 D(G(z)): 0.318/0.061\n",
      "[4/50][101/843] Loss_D: 0.314 Loss_G: 1.267 D(x): 1.156 D(G(z)): 0.502/-0.058\n",
      "[4/50][201/843] Loss_D: 0.092 Loss_G: 0.551 D(x): 0.985 D(G(z)): 0.163/0.084\n",
      "[4/50][301/843] Loss_D: 0.102 Loss_G: 1.121 D(x): 0.885 D(G(z)): 0.132/0.096\n",
      "[4/50][401/843] Loss_D: 0.152 Loss_G: 1.460 D(x): 1.080 D(G(z)): 0.355/-0.074\n",
      "D_mean is [3.2306833267211914, 1.2629009070694446, 0.7730329616516829, 0.5699673144451032]\n",
      "G_mean is [16.984115600585938, 1.9499318293072283, 1.3754797047954053, 1.151490948677063]\n",
      "[4/50][501/843] Loss_D: 0.090 Loss_G: 0.717 D(x): 0.918 D(G(z)): 0.251/0.030\n",
      "[4/50][601/843] Loss_D: 0.087 Loss_G: 0.483 D(x): 0.866 D(G(z)): 0.062/0.076\n",
      "[4/50][701/843] Loss_D: 0.034 Loss_G: 0.679 D(x): 0.921 D(G(z)): 0.119/0.133\n",
      "[4/50][801/843] Loss_D: 0.090 Loss_G: 0.816 D(x): 0.928 D(G(z)): 0.322/0.217\n",
      "[5/50][1/843] Loss_D: 0.198 Loss_G: 0.547 D(x): 0.699 D(G(z)): -0.185/0.254\n",
      "[5/50][101/843] Loss_D: 0.161 Loss_G: 0.528 D(x): 0.824 D(G(z)): 0.025/0.414\n",
      "[5/50][201/843] Loss_D: 0.104 Loss_G: 1.044 D(x): 1.071 D(G(z)): 0.180/-0.217\n",
      "[5/50][301/843] Loss_D: 0.233 Loss_G: 0.675 D(x): 0.681 D(G(z)): -0.100/0.325\n",
      "[5/50][401/843] Loss_D: 0.149 Loss_G: 0.812 D(x): 1.202 D(G(z)): 0.466/0.082\n",
      "[5/50][501/843] Loss_D: 0.051 Loss_G: 0.657 D(x): 0.775 D(G(z)): -0.008/0.109\n",
      "[5/50][601/843] Loss_D: 0.043 Loss_G: 0.392 D(x): 0.970 D(G(z)): 0.202/0.439\n",
      "D_mean is [3.2306833267211914, 1.2629009070694446, 0.7730329616516829, 0.5699673144451032, 0.4598786811286118]\n",
      "G_mean is [16.984115600585938, 1.9499318293072283, 1.3754797047954053, 1.151490948677063, 1.0330863677412272]\n",
      "[5/50][701/843] Loss_D: 0.163 Loss_G: 0.531 D(x): 1.154 D(G(z)): 0.168/0.056\n",
      "[5/50][801/843] Loss_D: 0.079 Loss_G: 0.932 D(x): 1.075 D(G(z)): 0.336/-0.026\n",
      "[6/50][1/843] Loss_D: 0.041 Loss_G: 0.274 D(x): 0.754 D(G(z)): 0.022/0.311\n",
      "[6/50][101/843] Loss_D: 0.173 Loss_G: 0.462 D(x): 1.111 D(G(z)): 0.303/0.041\n",
      "[6/50][201/843] Loss_D: 0.119 Loss_G: 0.385 D(x): 1.026 D(G(z)): 0.269/0.107\n",
      "[6/50][301/843] Loss_D: 0.050 Loss_G: 0.441 D(x): 0.931 D(G(z)): 0.109/0.111\n",
      "[6/50][401/843] Loss_D: 0.065 Loss_G: 0.599 D(x): 0.875 D(G(z)): 0.090/0.039\n",
      "[6/50][501/843] Loss_D: 0.016 Loss_G: 0.618 D(x): 0.896 D(G(z)): 0.171/0.189\n",
      "[6/50][601/843] Loss_D: 0.026 Loss_G: 0.413 D(x): 0.705 D(G(z)): 0.117/0.174\n",
      "[6/50][701/843] Loss_D: 0.033 Loss_G: 0.518 D(x): 0.919 D(G(z)): 0.154/0.233\n",
      "D_mean is [3.2306833267211914, 1.2629009070694446, 0.7730329616516829, 0.5699673144451032, 0.4598786811286118, 0.39109048181325196]\n",
      "G_mean is [16.984115600585938, 1.9499318293072283, 1.3754797047954053, 1.151490948677063, 1.0330863677412272, 0.9591346399486065]\n",
      "[6/50][801/843] Loss_D: 0.240 Loss_G: 0.746 D(x): 0.759 D(G(z)): -0.050/0.281\n",
      "[7/50][1/843] Loss_D: 0.100 Loss_G: 1.057 D(x): 1.003 D(G(z)): 0.133/-0.284\n",
      "[7/50][101/843] Loss_D: 0.107 Loss_G: 0.546 D(x): 0.967 D(G(z)): 0.284/0.442\n",
      "[7/50][201/843] Loss_D: 0.041 Loss_G: 1.379 D(x): 1.051 D(G(z)): 0.139/-0.069\n",
      "[7/50][301/843] Loss_D: 0.308 Loss_G: 0.829 D(x): 1.119 D(G(z)): 0.456/-0.043\n",
      "[7/50][401/843] Loss_D: 0.059 Loss_G: 1.052 D(x): 1.198 D(G(z)): 0.366/0.121\n",
      "[7/50][501/843] Loss_D: 0.137 Loss_G: 0.776 D(x): 0.807 D(G(z)): 0.006/0.128\n",
      "[7/50][601/843] Loss_D: 0.326 Loss_G: 0.950 D(x): 0.683 D(G(z)): -0.043/0.226\n"
     ]
    }
   ],
   "source": [
    "# 学習のループ\n",
    "n_epoch = 50\n",
    "D_loss = []\n",
    "G_loss = []\n",
    "D_mean = []\n",
    "G_mean = []\n",
    "# D_loss_var = []\n",
    "for epoch in range(n_epoch):\n",
    "    #D_loss_sum = 0\n",
    "    #G_loss_sum = 0\n",
    "    for itr, data in enumerate(dataloaders['train']):\n",
    "        real_image = data[0].to(device)     # 元画像\n",
    "        #print(real_image)\n",
    "        sample_size = real_image.size(0)    # 画像枚数\n",
    "        noise = torch.randn(sample_size, nz, 1, device=device)   # 正規分布からノイズを生成\n",
    "        \n",
    "        real_target = torch.full((sample_size,), random.uniform(0.7, 1.2), device=device)     # 元画像に対する識別信号の目標値「1」\n",
    "        fake_target = torch.full((sample_size,), random.uniform(0.0, 0.3), device=device)     # 贋作画像に対する識別信号の目標値「0」\n",
    "        \n",
    "        ############################\n",
    "        # 識別器Dの更新\n",
    "        ###########################\n",
    "        netD.zero_grad()    # 勾配の初期化\n",
    "\n",
    "        output = netD(real_image)   # 識別器Dで元画像に対する識別信号を出力\n",
    "        errD_real = criterion(output, real_target)  # 元画像に対する識別信号の損失値\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        fake_image = netG(noise)    # 生成器Gでノイズから贋作画像を生成\n",
    "        #print(fake_image.detach())\n",
    "        output = netD(fake_image.detach())  # 識別器Dで元画像に対する識別信号を出力\n",
    "        errD_fake = criterion(output, fake_target)  # 贋作画像に対する識別信号の損失値\n",
    "        D_G_z1 = output.mean().item()\n",
    "\n",
    "        errD = errD_real + errD_fake    # 識別器Dの全体の損失\n",
    "        D_loss.append(errD.cpu().detach().numpy()) #DiscreminatorのLossを格納\n",
    "        errD.backward()    # 誤差逆伝播\n",
    "        optimizerD.step()   # Dのパラメーターを更新\n",
    "\n",
    "        ############################\n",
    "        # 生成器Gの更新\n",
    "        ###########################\n",
    "        netG.zero_grad()    # 勾配の初期化\n",
    "        \n",
    "        output = netD(fake_image)   # 更新した識別器Dで改めて贋作画像に対する識別信号を出力\n",
    "        errG = criterion(output, real_target)   # 生成器Gの損失値。Dに贋作画像を元画像と誤認させたいため目標値は「1」\n",
    "        #G_loss_sum += errG\n",
    "        G_loss.append(errG.cpu().detach().numpy()) #GeneratorのLossを格納\n",
    "        #print(errG.cpu().detach().numpy())\n",
    "        #print(\"G_sum is\",G_sum)\n",
    "        errG.backward()     # 誤差逆伝播\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()   # Gのパラメータを更新\n",
    "\n",
    "        if itr % display_interval == 0: \n",
    "            print('[{}/{}][{}/{}] Loss_D: {:.3f} Loss_G: {:.3f} D(x): {:.3f} D(G(z)): {:.3f}/{:.3f}'\n",
    "                  .format(epoch + 1, n_epoch,\n",
    "                          itr + 1, len(dataloaders['train']),\n",
    "                          errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "            \n",
    "        if epoch == 0 and itr == 0:     # 初回に元画像を保存する\n",
    "            #sem.vec2seq_save(\"real_pos_seq\", real_image.cpu(), 1)\n",
    "            D_mean.append(mean(list(map(float, D_loss))))\n",
    "            G_mean.append(mean(list(map(float, G_loss))))\n",
    "            \n",
    "        if len(D_loss) % 1000 == 0:\n",
    "            D_mean.append(mean(list(map(float, D_loss))))\n",
    "            G_mean.append(mean(list(map(float, G_loss))))\n",
    "            print(\"D_mean is\",D_mean)\n",
    "            print(\"G_mean is\",G_mean)\n",
    "        #print(\"[\",itr,\"] SUM_lossD:\",D_loss_sum)\n",
    "        #print(\"[\",itr,\"] SUM_lossG:\",G_loss_sum)\n",
    "    ############################\n",
    "    # Lossの保存\n",
    "    ############################\n",
    "    #print(\"epoch\",epoch+1,\"D_loss is\",D_loss)\n",
    "    #print(\"epoch\",epoch+1,\"D_mean is\",mean(list(map(float, D_sum))))\n",
    "    #print(\"epoch\",epoch+1,\"G_loss is\",G_loss)\n",
    "    #print(\"epoch\",epoch+1,\"G_mean is\",mean(list(map(float, G_sum))))\n",
    "    #D_loss_var.append(variance(list(map(float, D_sum))))\n",
    "    #print(\"D_losses\",D_loss)\n",
    "    #print(\"G_losses\",G_loss)\n",
    "    #print(fake_image.detach())\n",
    "#     vutils.save_image(fake_image.detach(), '{}/fake_samples_epoch_{:03d}.png'.format(outf, epoch + 1),\n",
    "#                       normalize=True, nrow=10)\n",
    "\n",
    "    ############################\n",
    "    # 確認用の配列生成とその配列の保存\n",
    "    ############################\n",
    "        \n",
    "#     if (epoch + 1) % 5 == 0:   # 5エポックごとにfakeseqを保存する\n",
    "#         filename = 'fake_pos_seq_epoch' + str(epoch + 1)\n",
    "#         fake_image = netG(fixed_noise)  # 1エポック終了ごとに確認用の贋作配列を生成する\n",
    "#         sem.vec2seq_save(filename, fake_image.cpu().detach(), batch_size)\n",
    "#         torch.save(netG.state_dict(), '{}/netG_epoch_{}.pth'.format(outf, epoch + 1))\n",
    "#         torch.save(netD.state_dict(), '{}/netD_epoch_{}.pth'.format(outf, epoch + 1))\n",
    "\n",
    "############################\n",
    "# 結果の表示\n",
    "############################\n",
    "\n",
    "x_num = list(map(lambda x: x*1000, np.arange(0,len(D_mean))))\n",
    "#print(x_num)\n",
    "matplotlib.style.use('ggplot')\n",
    "fig1, Dx = plt.subplots()\n",
    "Dx.set_xlabel('iteration')\n",
    "Dx.set_ylabel('Dis/Loss')\n",
    "Dx.plot(x_num,D_mean,label='Discriminator Loss')\n",
    "Dx.legend(loc = 'best')\n",
    "Dx.set_title('')\n",
    "\n",
    "fig2, Gx = plt.subplots()\n",
    "Gx.set_xlabel('iteration')\n",
    "Gx.set_ylabel('Gen/Loss')\n",
    "Gx.plot(x_num,G_mean,label='Generater Loss')\n",
    "Gx.legend(loc = 'best')\n",
    "Gx.set_xlim(0, 54000)\n",
    "\n",
    "plt.xticks(np.arange(0, 540 + 1, 10000))\n",
    "Gx.set_title('')\n",
    "plt.title('')\n",
    "\n",
    "plt.show\n",
    "\n",
    "fig1.savefig('iterDLosses.png')\n",
    "fig2.savefig('iterGLosses.png')\n",
    "\n",
    "print(\"Discriminator's variance is\",variance(list(map(float, D_loss))))\n",
    "print(\"Generater's variance is\",variance(list(map(float, G_loss))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jIOzZdu6zyw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSGAN_ver2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
